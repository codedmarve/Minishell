
LEXER & PARSER
A lexer uses regular expressions to convert each syntactical element from the input into a token, 
essentially mapping the input to a stream of tokens. A parser reads in a stream of tokens and attempts 
to match tokens to a set of rules, where the end result maps the token stream to an abstract syntax tree.


https://www.cse.chalmers.se/edu/year/2015/course/DAT150/lectures/proglang-04.html
The lexer splits the code into tokens.
The lexer also classifies each token into classes. For example, the string
    i2=i1+271
results in the following list of classified tokens:
    identifier "i1"
    symbol "="
    identifier "i2"
    symbol "+"
    integer "271"
The lexer should read the source code character by character, and send tokens to the parser.
After each token, it should use the next character cto decide what kind of token to read.
if c is a digit, collect an integer as long as you read digits
if c is a letter, collect an identifier as long as you read identifier characters (digit, letter, ')
if c is a double quote, collect a string literal as long as you read characters other than a double quote
if c is space character (i.e. space, newline, or tab), ignore it and read next character
Longest match: read a token as long as you get characters that can belong to it.

The lexer will get the input from the prompt once the user pressed enter and create so called lexer_tokens from it
The lexer_tokens are a 2D char array char *lexer_tokens[]
Now the given input i.e. cat <<eof >file1 && cat file1 && abc || wc <file1 | cat >file2 will result in lexer_tokens like:
lex_tok[0] cat	
lex_tok[1]	<<eof
lex_tok[2]	>file
lex_tok[3]	&&
lex_tok[4]	cat
lex_tok[5]	file1
lex_tok[6]	&&
lex_tok[7]	abc
lex_tok[8]	||
lex_tok[9]	wc
lex_tok[10] file1
lex_tok[11] |
lex_tok[12] cat
lex_tok[13] >file2